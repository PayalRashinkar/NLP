{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7fa547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4ab6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_sequences = []\n",
    "        self.word_set = set()\n",
    "        self.tag_set = set()\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.file_path, 'r') as dataset_file:\n",
    "            lines = dataset_file.readlines()\n",
    "        \n",
    "        temp_words, temp_tags = [], []\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                if temp_words and temp_tags:\n",
    "                    self._update_sets(temp_words, temp_tags)\n",
    "                temp_words, temp_tags = [], []\n",
    "            else:\n",
    "                _, word, tag = line.strip().split()\n",
    "                temp_words.append(word)\n",
    "                temp_tags.append(tag)\n",
    "        \n",
    "        if temp_words and temp_tags:\n",
    "            self._update_sets(temp_words, temp_tags)\n",
    "        \n",
    "        return self.dataset_sequences, self.word_set, self.tag_set\n",
    "\n",
    "    def _update_sets(self, words, tags):\n",
    "        self.dataset_sequences.append((words, tags))\n",
    "        self.word_set.update(words)\n",
    "        self.tag_set.update(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509345de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDataLoader(Dataset):\n",
    "    def __init__(self, dataset, modification=None):\n",
    "        self.dataset = dataset\n",
    "        self.modification = modification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.numpy()\n",
    "\n",
    "        data_point = self.dataset[index]\n",
    "\n",
    "        if self.modification:\n",
    "            data_point = self.modification(data_point)\n",
    "\n",
    "        return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f548d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/train\"\n",
    "loader = DatasetLoader(file_path)\n",
    "sequences, unique_words, unique_tags = loader.load_data()\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in sequences]\n",
    "train_dataset = EnhancedDataLoader(tokenized_data)\n",
    "\n",
    "file_path = \"data/dev\"\n",
    "loader = DatasetLoader(file_path)\n",
    "sequences, unique_words, unique_tags = loader.load_data()\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in sequences]\n",
    "dev_dataset = EnhancedDataLoader(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18375b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_mappings(dataset, distinct_tags, min_freq):\n",
    "    word_counts = Counter(word for sentence, _ in dataset for word in sentence)\n",
    "    vocab_words = [word for word, freq in word_counts.items() if freq >= min_freq]\n",
    "\n",
    "    idx_to_word = {idx + 4: word for idx, word in enumerate(vocab_words)}\n",
    "    idx_to_word.update({0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>'})\n",
    "\n",
    "    idx_to_tag = {idx + 3: tag for idx, tag in enumerate(distinct_tags)}\n",
    "    idx_to_tag.update({0: '<pad>', 1: '<s>', 2: '</s>'})\n",
    "\n",
    "    return {v: k for k, v in idx_to_word.items()}, {v: k for k, v in idx_to_tag.items()}\n",
    "\n",
    "word2idx, tag2idx = build_vocab_mappings(sequences, unique_tags, min_freq=1)\n",
    "vocab_size = len(word2idx)\n",
    "num_tags = len(tag2idx)\n",
    "\n",
    "def sequence_padding(data_batch, word_to_idx, tag_to_idx, padding='<pad>', start='<s>', stop='</s>', unknown='<unk>'):\n",
    "    max_sequence_length = max(len(sentence) + 2 for sentence, _ in data_batch)\n",
    "\n",
    "    sequence_word_pads = []\n",
    "    sequence_tag_pads = []\n",
    "\n",
    "    for sentence, labels in data_batch:\n",
    "        padded_sentence = [start] + sentence + [stop]\n",
    "        padded_sentence = [word_to_idx.get(word, word_to_idx[unknown]) for word in padded_sentence] + [word_to_idx[padding]] * (max_sequence_length - len(padded_sentence))\n",
    "        sequence_word_pads.append(padded_sentence)\n",
    "\n",
    "        padded_labels = [start] + labels + [stop]\n",
    "        padded_labels = [tag_to_idx[label] for label in padded_labels] + [tag_to_idx[padding]] * (max_sequence_length - len(padded_labels))\n",
    "        sequence_tag_pads.append(padded_labels)\n",
    "\n",
    "    return torch.tensor(sequence_word_pads, dtype=torch.long), torch.tensor(sequence_tag_pads, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: sequence_padding(batch, word2idx, tag2idx),\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: sequence_padding(batch, word2idx, tag2idx),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c91fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, data_loader, criterion, total_tags):\n",
    "    model.eval()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    actual_tags = []\n",
    "    predicted_tags = []\n",
    "\n",
    "    aggregate_accuracy = 0\n",
    "    batch_count = 0\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            texts, labels = batch\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(texts)\n",
    "            predictions = predictions.reshape(-1, total_tags)\n",
    "            labels = labels.flatten()\n",
    "\n",
    "            batch_loss = criterion(predictions, labels)\n",
    "            cumulative_loss += batch_loss.item()\n",
    "\n",
    "            actual = labels.cpu().numpy()\n",
    "            predicted = torch.argmax(predictions, axis=1).cpu().numpy()\n",
    "            actual_tags.extend(actual)\n",
    "\n",
    "            _, predicted_labels = torch.max(predictions, axis=1)\n",
    "            predicted_tags.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "            valid_predictions = actual != 0\n",
    "            correct_preds = (predicted[valid_predictions] == actual[valid_predictions]).sum()\n",
    "            batch_accuracy = correct_preds / len(actual[valid_predictions])\n",
    "            \n",
    "            aggregate_accuracy += batch_accuracy\n",
    "            cumulative_loss += batch_loss\n",
    "            batch_count += 1\n",
    "\n",
    "    avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(\n",
    "        actual_tags,\n",
    "        predicted_tags,\n",
    "        average='macro',\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    return (cumulative_loss/batch_count), (aggregate_accuracy/batch_count) * 100, avg_precision * 100, avg_recall * 100, avg_f1 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMModel(nn.Module):\n",
    "    def __init__(self, total_vocab_size, output_dimension, embedding_dimension, lstm_hidden_dimension, lstm_layers, dropout_rate):\n",
    "        super(BidirectionalLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(total_vocab_size, embedding_dimension)\n",
    "        self.bi_lstm = nn.LSTM(embedding_dimension, lstm_hidden_dimension, lstm_layers, bidirectional=True, batch_first=True)\n",
    "        self.dense1 = nn.Linear(lstm_hidden_dimension * 2, output_dimension)\n",
    "        self.activation = nn.ELU()\n",
    "        self.regularization = nn.Dropout(dropout_rate)\n",
    "        self.dense2 = nn.Linear(output_dimension, num_tags)\n",
    "\n",
    "    def forward(self, text_sequence):\n",
    "        embedded_text = self.embed(text_sequence)\n",
    "        lstm_out, _ = self.bi_lstm(embedded_text)\n",
    "        dense_out = self.dense1(lstm_out)\n",
    "        activated_out = self.activation(dense_out)\n",
    "        dropped_out = self.regularization(activated_out)\n",
    "        final_logits = self.dense2(dropped_out)\n",
    "\n",
    "        return final_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08004541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/payalrashinkar/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.3405, Val Loss: 0.5501, Accuracy: 92.7310, Precision: 68.3543, Recall 62.9597, F1_score 63.4775\n",
      "Epoch 2/15, Train Loss: 0.1843, Val Loss: 0.4890, Accuracy: 94.1560, Precision: 77.1547, Recall 65.8963, F1_score 68.6472\n",
      "Epoch 3/15, Train Loss: 0.1516, Val Loss: 0.4919, Accuracy: 94.3130, Precision: 75.2650, Recall 67.4924, F1_score 68.1832\n",
      "Epoch 4/15, Train Loss: 0.1375, Val Loss: 0.4416, Accuracy: 94.9268, Precision: 74.8177, Recall 69.7121, F1_score 69.9858\n",
      "Epoch 5/15, Train Loss: 0.1277, Val Loss: 0.4870, Accuracy: 94.8921, Precision: 76.6437, Recall 69.6529, F1_score 70.6629\n",
      "Epoch 6/15, Train Loss: 0.1221, Val Loss: 0.3907, Accuracy: 95.4052, Precision: 68.4027, Recall 72.8155, F1_score 68.0766\n",
      "Epoch 7/15, Train Loss: 0.1150, Val Loss: 0.3889, Accuracy: 95.7514, Precision: 70.5640, Recall 73.5386, F1_score 66.5893\n",
      "Epoch 8/15, Train Loss: 0.1083, Val Loss: 0.3911, Accuracy: 95.5912, Precision: 76.9790, Recall 73.1357, F1_score 71.1219\n",
      "Epoch 9/15, Train Loss: 0.1008, Val Loss: 0.4327, Accuracy: 95.7594, Precision: 71.0620, Recall 72.9383, F1_score 67.3529\n",
      "Epoch 10/15, Train Loss: 0.0960, Val Loss: 0.3164, Accuracy: 96.1178, Precision: 71.3573, Recall 75.6930, F1_score 67.1275\n",
      "Epoch 11/15, Train Loss: 0.0899, Val Loss: 0.3865, Accuracy: 96.1128, Precision: 71.8945, Recall 73.4609, F1_score 66.5850\n",
      "Epoch 12/15, Train Loss: 0.0845, Val Loss: 0.3438, Accuracy: 96.5146, Precision: 64.3458, Recall 75.4740, F1_score 61.7849\n",
      "Epoch 13/15, Train Loss: 0.0843, Val Loss: 0.3175, Accuracy: 96.5417, Precision: 69.0453, Recall 75.6383, F1_score 66.8687\n",
      "Epoch 14/15, Train Loss: 0.0801, Val Loss: 0.4274, Accuracy: 96.2915, Precision: 61.8724, Recall 74.1335, F1_score 61.3371\n",
      "Epoch 15/15, Train Loss: 0.0783, Val Loss: 0.3236, Accuracy: 96.4886, Precision: 71.1227, Recall 75.6599, F1_score 66.8304\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "num_tags = len(tag2idx)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "model = BidirectionalLSTMModel(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag2idx['<pad>']) \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.25, momentum=0.9, weight_decay=0.00005)  # TODO add parameters\n",
    "\n",
    "patience = 6\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "clip_value = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        word_seqs, tag_seqs = batch\n",
    "        word_seqs = word_seqs.to(device)\n",
    "        tag_seqs = tag_seqs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(word_seqs)\n",
    "        logits = logits.view(-1, num_tags)\n",
    "        tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "        loss = loss_function(logits, tag_seqs)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * word_seqs.size(0)\n",
    "        total_samples += word_seqs.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / total_samples\n",
    "\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score = evaluate_model_performance(model, dev_loader, loss_function, num_tags)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        final_model = model\n",
    "        torch.save(model.state_dict(), \"blstm1.pt\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall {val_recall:.4f}, F1_score {val_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73987a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BidirectionalLSTMModel(\n",
       "  (embed): Embedding(9971, 100)\n",
       "  (bi_lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (dense1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (activation): ELU(alpha=1.0)\n",
       "  (regularization): Dropout(p=0.33, inplace=False)\n",
       "  (dense2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = BidirectionalLSTMModel(9971, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "saved_state_dict = torch.load(\"blstm1.pt\")\n",
    "loaded_model.load_state_dict(saved_state_dict)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb87398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputfile(model, textFile, outputFile, has_tags=True):\n",
    "    with open(textFile, 'r') as input_file, open(outputFile, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        tags = [] if has_tags else None  \n",
    "        pad_token='<pad>'\n",
    "        init_token='<s>'\n",
    "        eos_token='</s>'\n",
    "        unk_token='<unk>'\n",
    "        \n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "               \n",
    "                if len(words) > 0:\n",
    "                    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    model.eval()\n",
    "                    tokens = new_text.split()\n",
    "                    padded_tokens = [init_token] + tokens + [eos_token]\n",
    "                    indices = [word2idx.get(word, word2idx[unk_token]) for word in padded_tokens]\n",
    "                    input_tensor = torch.tensor([indices]).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(input_tensor)\n",
    "                    \n",
    "                    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "                    predicted_tags = [idx2tag.get(idx, unk_token) for idx in predicted_indices][1:-1]\n",
    "\n",
    "                    for i in range(len(indexs)):\n",
    "                        index = indexs[i]\n",
    "                        word = words[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "\n",
    "                        predictionLine = f\"{index} {word} {prediction}\\n\"\n",
    "\n",
    "                        output_file.write(predictionLine)\n",
    "                    \n",
    "                    indexs = []\n",
    "                    words = []\n",
    "                    if has_tags: tags = []  \n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "               \n",
    "                split_line = line.strip().split()\n",
    "                index, word = split_line[:2]\n",
    "                indexs.append(index)\n",
    "                words.append(word)\n",
    "                if has_tags: tags.append(split_line[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9809f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outputfile(loaded_model, \"data/dev\", \"dev1.out\", has_tags=True)\n",
    "get_outputfile(loaded_model, \"data/test\", \"test1.out\", has_tags=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
