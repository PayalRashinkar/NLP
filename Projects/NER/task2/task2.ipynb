{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5553ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "glove_file = \"./glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42338f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_sequences = []\n",
    "        self.word_set = set()\n",
    "        self.tag_set = set()\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        temp_words, temp_tags = [], []\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                if temp_words and temp_tags:\n",
    "                    self.dataset_sequences.append((temp_words, temp_tags))\n",
    "                    self.word_set.update(temp_words)\n",
    "                    self.tag_set.update(temp_tags)\n",
    "                temp_words, temp_tags = [], []\n",
    "            else:\n",
    "                _, word, tag = line.strip().split()\n",
    "                temp_words.append(word)\n",
    "                temp_tags.append(tag)\n",
    "\n",
    "        if temp_words and temp_tags:\n",
    "            self.dataset_sequences.append((temp_words, temp_tags))\n",
    "            self.word_set.update(temp_words)\n",
    "            self.tag_set.update(temp_tags)\n",
    "        \n",
    "        return self.dataset_sequences, self.word_set, self.tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0061dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDataLoader(Dataset):\n",
    "    def __init__(self, dataset, modification=None):\n",
    "        self.dataset = dataset\n",
    "        self.modification = modification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.numpy()\n",
    "\n",
    "        data_point = self.dataset[index]\n",
    "\n",
    "        if self.modification:\n",
    "            data_point = self.modification(data_point)\n",
    "\n",
    "        return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c043466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/train\"\n",
    "loader = DatasetLoader(file_path)\n",
    "sequences, unique_words, unique_tags = loader.load_data()\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in sequences]\n",
    "train_dataset = EnhancedDataLoader(tokenized_data)\n",
    "\n",
    "file_path = \"data/dev\"\n",
    "loader = DatasetLoader(file_path)\n",
    "sequences, unique_words, unique_tags = loader.load_data()\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in sequences]\n",
    "dev_dataset = EnhancedDataLoader(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fbd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_mappings(dataset, distinct_tags, min_freq):\n",
    "    word_counts = Counter(word.lower() for sentence, _ in dataset for word in sentence)\n",
    "    vocab_words = [word for word, freq in word_counts.items() if freq >= min_freq]\n",
    "\n",
    "    idx_to_word = {idx + 4: word for idx, word in enumerate(vocab_words)}\n",
    "    idx_to_word.update({0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>'})\n",
    "\n",
    "    idx_to_tag = {idx + 3: tag for idx, tag in enumerate(distinct_tags)}\n",
    "    idx_to_tag.update({0: '<pad>', 1: '<s>', 2: '</s>'})\n",
    "\n",
    "    word2idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "    tag2idx = {tag: idx for idx, tag in idx_to_tag.items()}\n",
    "\n",
    "    return word2idx, tag2idx\n",
    "\n",
    "word2idx, tag2idx = build_vocab_mappings(sequences, unique_tags, 1)\n",
    "\n",
    "def pad_sequences(batch, word2idx, tag2idx, pad_token='<pad>', init_token='<s>', eos_token='</s>', unk_token='<unk>'):\n",
    "    max_len = max([len(seq) + 2 for seq, _ in batch])  # Add 2 to account for <s> and </s> tokens\n",
    "\n",
    "    padded_word_seqs = []\n",
    "    padded_upper_seqs = []\n",
    "    padded_tag_seqs = []\n",
    "\n",
    "    for words, tags in batch:\n",
    "        lower_words = [word.lower() for word in words]\n",
    "\n",
    "        padded_words = [init_token] + lower_words + [eos_token]\n",
    "        padded_words = [word2idx.get(word, word2idx[unk_token]) for word in padded_words] + [word2idx[pad_token]] * (max_len - len(padded_words))\n",
    "        padded_word_seqs.append(padded_words)\n",
    "\n",
    "        padded_uppers = [0] + [int(word[0].isupper()) for word in words] + [0] + [0] * (max_len - len(words) - 2)\n",
    "        padded_upper_seqs.append(padded_uppers)\n",
    "\n",
    "        padded_tags = [init_token] + tags + [eos_token]\n",
    "        padded_tags = [tag2idx[tag] for tag in padded_tags] + [tag2idx[pad_token]] * (max_len - len(padded_tags))\n",
    "        padded_tag_seqs.append(padded_tags)\n",
    "\n",
    "    return torch.tensor(padded_word_seqs), torch.tensor(padded_upper_seqs), torch.tensor(padded_tag_seqs)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word2idx, tag2idx),\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word2idx, tag2idx),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cba4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, word2idx, embedding_dim=100):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            idx = word2idx.get(word)\n",
    "            if idx is not None:\n",
    "                embeddings[idx] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac1b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "num_tags = len(tag2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, pretrained_embeddings=None):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False  # Optionally make embeddings non-trainable\n",
    "        \n",
    "        self.upper_embedding = nn.Embedding(2, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, upper_x):\n",
    "        x = self.embedding(x)\n",
    "        upper_x = self.upper_embedding(upper_x)\n",
    "        x = torch.cat([x, upper_x], dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2badeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = load_glove_embeddings(glove_file, word2idx, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9117b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC': 3, 'B-LOC': 4, 'I-LOC': 5, 'O': 6, 'I-ORG': 7, 'B-MISC': 8, 'B-PER': 9, 'I-PER': 10, 'B-ORG': 11, '<pad>': 0, '<s>': 1, '</s>': 2}\n",
      "I-MISC 1\n",
      "B-LOC 1\n",
      "I-LOC 1\n",
      "O 0.7\n",
      "I-ORG 1\n",
      "B-MISC 1\n",
      "B-PER 1\n",
      "I-PER 1\n",
      "B-ORG 1\n",
      "<pad> 0\n",
      "<s> 0\n",
      "</s> 0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)\n",
    "\n",
    "weight_list = [0,0,0,1,1,1,0.7,1,1,1,1,1]\n",
    "\n",
    "for i,w in tag2idx.items():\n",
    "    print(i, weight_list[w])\n",
    "print(len(weight_list))\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad17dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, data_loader, criterion, total_tags):\n",
    "    model.eval()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    actual_tags = []\n",
    "    predicted_tags = []\n",
    "\n",
    "    aggregate_accuracy = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            word_seqs, upper_seqs, tag_seqs = batch\n",
    "            word_seqs = word_seqs.to(device)\n",
    "            upper_seqs = upper_seqs.to(device)\n",
    "            tag_seqs = tag_seqs.to(device)\n",
    "\n",
    "            predictions = model(word_seqs, upper_seqs)\n",
    "            predictions = predictions.view(-1, total_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "            batch_loss = criterion(predictions, tag_seqs)\n",
    "            cumulative_loss += batch_loss.item()\n",
    "\n",
    "            actual = tag_seqs.cpu().numpy()\n",
    "            predicted = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "            actual_tags.extend(actual)\n",
    "            predicted_tags.extend(predicted)\n",
    "\n",
    "            valid_predictions = actual != 0\n",
    "            correct_preds = (predicted[valid_predictions] == actual[valid_predictions]).sum()\n",
    "            batch_accuracy = correct_preds / len(actual[valid_predictions]) if len(actual[valid_predictions]) > 0 else 0\n",
    "            \n",
    "            aggregate_accuracy += batch_accuracy\n",
    "            batch_count += 1\n",
    "\n",
    "    avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(\n",
    "        actual_tags,\n",
    "        predicted_tags,\n",
    "        average='macro',\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    return (cumulative_loss/batch_count), (aggregate_accuracy/batch_count) * 100, avg_precision * 100, avg_recall * 100, avg_f1 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c2556b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/payalrashinkar/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2459, Val Loss: 0.1213, Accuracy: 84.8753, Precision: 62.2947, Recall 58.4078, F1_score 57.0176\n",
      "Epoch 2/20, Train Loss: 0.1536, Val Loss: 0.1154, Accuracy: 85.2457, Precision: 62.0269, Recall 63.0930, F1_score 60.9245\n",
      "Epoch 3/20, Train Loss: 0.1349, Val Loss: 0.0906, Accuracy: 85.4522, Precision: 62.9981, Recall 64.7260, F1_score 62.3492\n",
      "Epoch 4/20, Train Loss: 0.1282, Val Loss: 0.1048, Accuracy: 85.2527, Precision: 62.2371, Recall 63.5970, F1_score 61.1664\n",
      "Epoch 5/20, Train Loss: 0.1225, Val Loss: 0.1034, Accuracy: 85.4945, Precision: 63.4273, Recall 64.2320, F1_score 62.2214\n",
      "Epoch 6/20, Train Loss: 0.1156, Val Loss: 0.0976, Accuracy: 85.3752, Precision: 61.2149, Recall 66.1597, F1_score 62.2912\n",
      "Epoch 7/20, Train Loss: 0.1101, Val Loss: 0.0808, Accuracy: 85.6027, Precision: 63.0019, Recall 66.8859, F1_score 63.6103\n",
      "Epoch 8/20, Train Loss: 0.1051, Val Loss: 0.0878, Accuracy: 85.4985, Precision: 62.8421, Recall 65.7627, F1_score 62.8972\n",
      "Epoch 9/20, Train Loss: 0.1034, Val Loss: 0.0833, Accuracy: 85.6304, Precision: 61.3167, Recall 67.5988, F1_score 63.1294\n",
      "Epoch 10/20, Train Loss: 0.1043, Val Loss: 0.0836, Accuracy: 85.7535, Precision: 61.7459, Recall 67.1984, F1_score 63.0866\n",
      "Epoch 11/20, Train Loss: 0.0998, Val Loss: 0.0913, Accuracy: 85.4717, Precision: 61.8247, Recall 65.7327, F1_score 62.0925\n",
      "Epoch 12/20, Train Loss: 0.1011, Val Loss: 0.0899, Accuracy: 85.5092, Precision: 61.3287, Recall 67.9217, F1_score 63.1924\n",
      "Epoch 13/20, Train Loss: 0.1017, Val Loss: 0.1101, Accuracy: 85.2494, Precision: 62.8196, Recall 64.4450, F1_score 61.8574\n",
      "Epoch 14/20, Train Loss: 0.0713, Val Loss: 0.0741, Accuracy: 85.8592, Precision: 63.3020, Recall 67.6386, F1_score 64.1278\n",
      "Epoch 15/20, Train Loss: 0.0684, Val Loss: 0.0839, Accuracy: 85.8775, Precision: 63.6376, Recall 67.5056, F1_score 64.0150\n",
      "Epoch 16/20, Train Loss: 0.0682, Val Loss: 0.0789, Accuracy: 85.7643, Precision: 63.1486, Recall 67.7787, F1_score 64.1567\n",
      "Epoch 17/20, Train Loss: 0.0668, Val Loss: 0.0761, Accuracy: 85.9111, Precision: 63.6702, Recall 67.6625, F1_score 64.3485\n",
      "Epoch 18/20, Train Loss: 0.0649, Val Loss: 0.0836, Accuracy: 85.6328, Precision: 62.8295, Recall 67.8598, F1_score 63.9961\n",
      "Epoch 19/20, Train Loss: 0.0671, Val Loss: 0.0909, Accuracy: 85.5536, Precision: 61.2975, Recall 65.5574, F1_score 61.5591\n",
      "Epoch 20/20, Train Loss: 0.0640, Val Loss: 0.0850, Accuracy: 85.7330, Precision: 62.7488, Recall 67.1654, F1_score 63.6442\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "model = BiLSTM(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, pretrained_embeddings)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag2idx['<pad>'], weight=weight_tensor)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3, momentum=0.9, weight_decay=0.00005) \n",
    "\n",
    "patience = 5\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "clip_value = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        word_seqs, upper_seqs, tag_seqs = batch\n",
    "        word_seqs = word_seqs.to(device)\n",
    "        upper_seqs = upper_seqs.to(device)\n",
    "        tag_seqs = tag_seqs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(word_seqs, upper_seqs)\n",
    "        logits = logits.view(-1, num_tags)\n",
    "        tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "        loss = loss_function(logits, tag_seqs)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * word_seqs.size(0)\n",
    "        total_samples += word_seqs.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / total_samples\n",
    "\n",
    "    \n",
    "\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score = evaluate_model_performance(model, dev_loader, loss_function, num_tags)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        final_model = model\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall {val_recall:.4f}, F1_score {val_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f91f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"blstm2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e981aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(9007, 100)\n",
       "  (upper_embedding): Embedding(2, 100)\n",
       "  (lstm): LSTM(200, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = BiLSTM(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, pretrained_embeddings)\n",
    "\n",
    "saved_state_dict = torch.load(\"blstm2.pt\")\n",
    "loaded_model.load_state_dict(saved_state_dict)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211d8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFile(model, textFile, outputFile):\n",
    "    with open(textFile, 'r') as input_file, open(outputFile, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        pad_token='<pad>'\n",
    "        init_token='<s>'\n",
    "        eos_token='</s>'\n",
    "        unk_token='<unk>'\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if words:  \n",
    "                    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "                    new_text = \" \".join(words)\n",
    "                    model.eval()\n",
    "                    tokens = new_text.split()\n",
    "                    lower_tokens = new_text.lower().split()\n",
    "                    padded_tokens = [init_token] + lower_tokens + [eos_token]\n",
    "                    tokenized_input = [word2idx.get(word, word2idx[unk_token]) for word in padded_tokens]\n",
    "                    upper_input = [0] + [int(token[0].isupper()) for token in tokens] + [0]\n",
    "                    input_tensor = torch.tensor([tokenized_input]).to(device)\n",
    "                    upper_tensor = torch.tensor([upper_input]).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(input_tensor, upper_tensor)\n",
    "                    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "                    predicted_tags = [idx2tag[idx] for idx in predicted_indices][1:-1]\n",
    "\n",
    "                    for index, word, prediction in zip(indexs, words, predicted_tags):\n",
    "                        predictionLine = f\"{index} {word} {prediction}\\n\"\n",
    "                        output_file.write(predictionLine)\n",
    "\n",
    "                    indexs, words, tags = [], [], [] \n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word, tag = line.strip().split()\n",
    "                indexs.append(index)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "\n",
    "\n",
    "createFile(loaded_model, \"data/dev\", \"dev2.out\")\n",
    "createFile(loaded_model, \"data/dev\", \"test2.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
